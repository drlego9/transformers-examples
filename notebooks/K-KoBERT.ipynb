{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:00:14.907102Z",
     "start_time": "2019-11-14T11:00:12.335461Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from gluonnlp.vocab import BERTVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:00:15.630157Z",
     "start_time": "2019-11-14T11:00:15.618573Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_config = {\n",
    "    'attention_probs_dropout_prob': 0.1,\n",
    "    'hidden_act': 'gelu',\n",
    "    'hidden_dropout_prob': 0.1,\n",
    "    'hidden_size': 768,\n",
    "    'initializer_range': 0.02,\n",
    "    'intermediate_size': 3072,\n",
    "    'max_position_embeddings': 512,\n",
    "    'num_attention_heads': 12,\n",
    "    'num_hidden_layers': 12,\n",
    "    'type_vocab_size': 2,\n",
    "    'vocab_size': 8002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T11:00:30.308333Z",
     "start_time": "2019-11-14T11:00:30.305897Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../.kobert/kobert-8002-config.json', 'w') as f:\n",
    "    json.dump(bert_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.668860Z",
     "start_time": "2019-11-13T11:56:04.741790Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel(config=BertConfig.from_dict(bert_config))\n",
    "assert isinstance(bert_model, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.873090Z",
     "start_time": "2019-11-13T11:56:05.669929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights\n",
    "model_file = '../.kobert/pytorch_kobert_2439f391a6.params'\n",
    "assert os.path.exists(model_file)\n",
    "\n",
    "bert_model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.889236Z",
     "start_time": "2019-11-13T11:56:05.874490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys of vocab dictionary:\n",
      "- idx_to_token\n",
      "- token_to_idx\n",
      "- reserved_tokens\n",
      "- unknown_token\n",
      "- padding_token\n",
      "- bos_token\n",
      "- eos_token\n",
      "- mask_token\n",
      "- sep_token\n",
      "- cls_token\n"
     ]
    }
   ],
   "source": [
    "# load dictionary\n",
    "vocab_file = '../.kobert/kobertvocab_f38b8a4d6d.json'\n",
    "assert os.path.exists(vocab_file)\n",
    "\n",
    "with open(vocab_file, 'rt') as f:\n",
    "    vocab = json.load(f)\n",
    "    print('Available keys of vocab dictionary:\\n- ', end='')\n",
    "    print(*list(vocab.keys()), sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.920536Z",
     "start_time": "2019-11-13T11:56:05.890791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (gluonnlp, BERT): 8002\n"
     ]
    }
   ],
   "source": [
    "# gluonnlp tokenizer\n",
    "gluon_tokenizer = BERTVocab.from_json(open(vocab_file, 'rt').read())\n",
    "print(f\"Vocab size (gluonnlp, BERT): {len(gluon_tokenizer.idx_to_token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.925045Z",
     "start_time": "2019-11-13T11:56:05.921613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_to_token    : 8002\n",
      "token_to_idx    : 8002\n",
      "reserved_tokens : ['[MASK]', '[SEP]', '[CLS]']\n",
      "unknown_token   : [UNK]\n",
      "padding_token   : [PAD]\n",
      "bos_token       : None\n",
      "eos_token       : None\n",
      "mask_token      : [MASK]\n",
      "sep_token       : [SEP]\n",
      "cls_token       : [CLS]\n"
     ]
    }
   ],
   "source": [
    "for k, v in vocab.items():\n",
    "    if k in ['idx_to_token', 'token_to_idx']:\n",
    "        print(f\"{k:<16}: {len(v)}\")\n",
    "        continue\n",
    "    print(f\"{k:<16}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.942052Z",
     "start_time": "2019-11-13T11:56:05.926791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write to vocab to file (one token per line; huggingface format)\n",
    "vocab_file_huggingface = '../.kobert/kobert_vocab_huggingface_format.txt'\n",
    "with open(vocab_file_huggingface, 'wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(vocab.get('idx_to_token')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.967433Z",
     "start_time": "2019-11-13T11:56:05.943426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (huggingface, BERT): 8002\n"
     ]
    }
   ],
   "source": [
    "# Instantiate BertTokenizer\n",
    "tokenizer_configs = {\n",
    "    'vocab_file': vocab_file_huggingface,\n",
    "    'do_lower_case': False,\n",
    "    'do_basic_tokenize': True,\n",
    "    'never_split': None,\n",
    "    'unk_token': '[UNK]',\n",
    "    'sep_token': '[SEP]',\n",
    "    'pad_token': '[PAD]',\n",
    "    'cls_token': '[CLS]',\n",
    "    'mask_token': '[MASK]',\n",
    "    'tokenize_chinese_chars': False,\n",
    "}\n",
    "\n",
    "bert_tokenizer = BertTokenizer(**tokenizer_configs)\n",
    "print(f'Vocab size (huggingface, BERT): {bert_tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:05.981313Z",
     "start_time": "2019-11-13T11:56:05.968604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 4638: ▁카리스마 vs. ▁카리스마\n",
      "Index 0876: ▁거짓말 vs. ▁거짓말\n",
      "Index 1661: ▁대중 vs. ▁대중\n",
      "Index 0635: ▁A vs. ▁A\n",
      "Index 3642: ▁의한 vs. ▁의한\n",
      "Index 7047: 위는 vs. 위는\n",
      "Index 1680: ▁대학 vs. ▁대학\n",
      "Index 4375: ▁진행될 vs. ▁진행될\n",
      "Index 1464: ▁넘는 vs. ▁넘는\n",
      "Index 2972: ▁시리아 vs. ▁시리아\n"
     ]
    }
   ],
   "source": [
    "# Gluonnlp vs. Huggingface\n",
    "for i in np.random.randint(0, 8002, 10):\n",
    "    gluon_ver = gluon_tokenizer.idx_to_token[i]\n",
    "    huggingface_ver = bert_tokenizer.ids_to_tokens[i]\n",
    "    print(f\"Index {i:>04}: {gluon_ver} vs. {huggingface_ver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:06.004716Z",
     "start_time": "2019-11-13T11:56:05.982692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace '_' with '#' ('__' with '##')\n",
    "tokens = []\n",
    "for token in vocab['idx_to_token']:\n",
    "    if token == '▁':\n",
    "        tokens.append(token)\n",
    "    else:\n",
    "        token = token.replace('▁', '##')\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:06.024628Z",
     "start_time": "2019-11-13T11:56:06.005799Z"
    }
   },
   "outputs": [],
   "source": [
    "new_vocab_file_huggingface = '../.kobert/new_kobert_vocab_huggingface_format.txt'\n",
    "with open(new_vocab_file_huggingface, 'wt', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:06.046007Z",
     "start_time": "2019-11-13T11:56:06.025976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload BERT tokenizer\n",
    "tokenizer_configs['vocab_file'] = new_vocab_file_huggingface\n",
    "bert_tokenizer = BertTokenizer(**tokenizer_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:06.058434Z",
     "start_time": "2019-11-13T11:56:06.046953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 7499: 칸 vs. 칸\n",
      "Index 1318: ▁김동 vs. ##김동\n",
      "Index 6006: 라도 vs. 라도\n",
      "Index 7399: 참여 vs. 참여\n",
      "Index 4439: ▁창단 vs. ##창단\n",
      "Index 2098: ▁무대 vs. ##무대\n",
      "Index 0854: ▁개최 vs. ##개최\n",
      "Index 4454: ▁채무 vs. ##채무\n",
      "Index 7080: 으로서 vs. 으로서\n",
      "Index 1810: ▁들어오 vs. ##들어오\n"
     ]
    }
   ],
   "source": [
    "# Gluonnlp vs. Huggingface (AGAIN!)\n",
    "for i in np.random.randint(0, 8002, 10):\n",
    "    gluon_ver = gluon_tokenizer.idx_to_token[i]\n",
    "    huggingface_ver = bert_tokenizer.ids_to_tokens[i]\n",
    "    print(f\"Index {i:>04}: {gluon_ver} vs. {huggingface_ver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:06.075027Z",
     "start_time": "2019-11-13T11:56:06.059548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "-> 그런데 왜 결과는 이런 식이야?\n",
      "Tokenized text:\n",
      "-> ['그런', '##데', '왜', '[UNK]', '[UNK]', '식', '##이', '##야', '?']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "text = \"그런데 왜 결과는 이런 식이야?\"\n",
    "tokenized_text = bert_tokenizer.tokenize(text)\n",
    "print(f\"Original text:\\n-> {text}\")\n",
    "print(f\"Tokenized text:\\n-> {tokenized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T11:56:06.095651Z",
     "start_time": "2019-11-13T11:56:06.076040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그런 ##데 왜 [UNK] [UNK] 식 ##이 ##야 ? "
     ]
    }
   ],
   "source": [
    "# Drill down\n",
    "for token in bert_tokenizer.basic_tokenizer.tokenize(text):\n",
    "    for sub_token in bert_tokenizer.wordpiece_tokenizer.tokenize(token):\n",
    "        print(sub_token, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T13:18:41.196104Z",
     "start_time": "2019-11-13T13:18:41.193548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write tokenizer configs to json file\n",
    "with open('../.kobert/kobert_tokenizer_config.json', 'w') as f:\n",
    "    json.dump(tokenizer_configs, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T07:11:39.942297Z",
     "start_time": "2019-11-14T07:11:38.556874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save BERT model using .save_pretrained -> for later usage\n",
    "save_dir = '../.kobert/pretrained/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "bert_model.save_pretrained(save_directory=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
